# deeplearning-study
deep-learning study 2023

Date | Paper | Presenter | Links
:---: | :---: | :---: | :---:
5/16 | Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework, ECCV 2022. | Hyungjun Lee | [paper](https://arxiv.org/abs/2203.11991) [review](https://notaai.notion.site/Joint-Feature-Learning-and-Relation-Modeling-for-Tracking-A-One-Stream-Framework-bd00a7851fa3467fb12b2a4ea705cb00)
6/08 | Deformable DETR: Deformable Transformers for End-to-End Object Detection, ICLR 2021. | In-Jae Lee | [paper](https://arxiv.org/abs/2010.04159) [review](https://confused-winter-bd1.notion.site/DEFORMABLE-DETR-DEFORMABLE-TRANSFORMERS-FOR-END-TO-END-OBJECT-DETECTION-e34951da4f7c43d4814f9faee915c8c9?pvs=25)

Paper List
Paper | Conference | Links
:---: | :---: | :---:
Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework. | ECCV 2022 | [paper](https://arxiv.org/abs/2203.11991)
Alpha-Refine: Boosting Tracking Performance by Precise Bounding Box Estimation. | CVPR 2021 | [paper](https://arxiv.org/abs/2012.06815)
Continual Test-Time Domain Adaptation. | CVPR 2022 | [paper](https://arxiv.org/abs/2203.13591)
EcoTTA: Memory-Efficient Continual Test-time Adaptation via Self-distilled Regularization. | CVPR 2023 | [paper](https://arxiv.org/abs/2303.01904)
Image as Set of Points.  | ICLR 2023 | [paper](https://arxiv.org/abs/2303.01494)
The Forward-Forward Algorithm: Some Preliminary Investigations | arXiv 2022 | [paper](https://arxiv.org/abs/2212.13345)
Segment Anything | arXiv 2023 | [paper](https://arxiv.org/abs/2304.02643)
How Do Vision Transformers Work? | ICLR 2022  | [paper](https://arxiv.org/abs/2202.06709)
DINOv2: Learning Robust Visual Features without Supervision | arXiv 2023  | [paper](https://arxiv.org/abs/2304.07193)



### Study Member
* [Hyungjun Lee](https://github.com/rhtm02)
* [Injae Lee](https://github.com/oliver0922)
* [Sunmyung Lee](https://github.com/leesunmyung)
* [Wonjune Kim](https://github.com/culigan3186)
